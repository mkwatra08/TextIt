{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkwatra08/TextIt/blob/main/Text_It.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers transformers accelerate\n",
        "!pip install torch --upgrade\n",
        "!wget http://images.cocodataset.org/zips/train2014.zip\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
        "!unzip train2014.zip\n",
        "!unzip annotations_trainval2014.zip\n",
        "\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
        "from torch import autocast\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torch import optim\n",
        "from PIL import Image\n",
        "\n",
        "# Load captions\n",
        "with open('annotations/captions_train2014.json', 'r') as f:\n",
        "    captions = json.load(f)\n",
        "\n",
        "# Extract image paths and captions\n",
        "image_folder = 'train2014'\n",
        "text_image_pairs = []\n",
        "\n",
        "for annot in captions['annotations']:\n",
        "    image_id = annot['image_id']\n",
        "    caption = annot['caption']\n",
        "    image_path = os.path.join(image_folder, f'COCO_train2014_{image_id:012d}.jpg')\n",
        "    text_image_pairs.append((caption, image_path))\n",
        "\n",
        "print(f\"Total pairs: {len(text_image_pairs)}\")\n",
        "print(f\"Example pair: {text_image_pairs[0]}\")\n",
        "\n",
        "class TextImageDataset(Dataset):\n",
        "    def __init__(self, text_image_pairs, tokenizer, transform=None):\n",
        "        self.text_image_pairs = text_image_pairs\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_image_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text, image_path = self.text_image_pairs[idx]\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        tokens = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n",
        "        return tokens.input_ids[0], tokens.attention_mask[0], image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "dataset = TextImageDataset(text_image_pairs, tokenizer, transform)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16 if device == \"cuda\" else torch.float32)\n",
        "pipe.to(device)\n",
        "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "num_epochs = 5\n",
        "optimizer = optim.AdamW(pipe.unet.parameters(), lr=1e-5)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for input_ids, attention_mask, images in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        images = images.to(device)\n",
        "        with autocast(device):\n",
        "            loss = pipe(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device), pixel_values=images).loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
        "\n",
        "pipe.save_pretrained(\"path_to_save_model\")\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"path_to_save_model\", torch_dtype=torch.float16 if device == \"cuda\" else torch.float32)\n",
        "pipe.to(device)\n",
        "\n",
        "prompt = \"A fantasy landscape with mountains and rivers at sunset\"\n",
        "with autocast(device):\n",
        "    image = pipe(prompt).images[0]\n",
        "\n",
        "image.show()\n"
      ],
      "metadata": {
        "id": "7tx8ib92VJrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers transformers accelerate\n",
        "!pip install torch --upgrade\n",
        "!pip install huggingface_hub\n",
        "\n",
        "from huggingface_hub import login\n",
        "from huggingface_hub import HfApi, HfFolder, Repository\n",
        "\n",
        "# save_directory = \"path_to_save_model\"\n",
        "# pipe.save_pretrained(save_directory)\n",
        "\n",
        "repo_name = \"pallavi13/Text-It\"\n",
        "api = HfApi()\n",
        "username = HfFolder.get_token().split(\":\")[0]\n",
        "\n",
        "repo_url = api.create_repo(name=repo_name, exist_ok=True)\n",
        "repo = Repository(local_dir=save_directory, clone_from=repo_url)\n",
        "repo.push_to_hub(commit_message=\"Initial commit\")\n"
      ],
      "metadata": {
        "id": "j-0DwcerH41-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dXJ-HgCHVIiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mcwWEqMNH7yJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}